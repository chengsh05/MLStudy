<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# MLStudy
了解信息时代最基本的几个概念，这些概念构成了整个信息时代的基石也是整个信息时代的核心。

## 信息量的衡量指标
###信息熵  
**H(x)表示用以消除这个事件的不确定性所需要的统计信息量即为信息熵。**

离散型信息熵的公式：  
变量取值为i到n，所有取值的熵值之和  

$$ H(X) = - \sum_{x = i}^n P(x_i)\log P(x_i)$$  

连续型信息熵的公式：  
P(x)：概率密度函数  
$$ H(x) = - \int_x P(x)\log (P(x))dx $$
对应的Python代码：
```python
import numpy as np
def ComputerEntropy(x, p_x):
      if(isinstance(p_x, float)):
            H_x = -p_x * np.log2(p_x)
            return x, H_x
#Test Case
x1, H_x1 = ComputerEntropy("532", 0.5)
x2, H_x2 = ComputerEntropy("531", 0.2)
x3, H_x3 = ComputerEntropy("530", 0.3)
print(x1, H_x1)
print(x2, H_x2)
print(x3, H_x3)
print("All H(x):", H_x1 + H_x2 + H_x3)
```

####联合熵
两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用\(H(X,Y)\)表示

####条件熵  
在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。
$$ H(Y|X) =  \sum_{x \in X}P(x)H(Y|X = x) =  - \sum_{x \in X}P(x) \sum_{y \in Y} P(y|x) log P(y|x)  = -\sum_{x \in X y \in Y} P(x,y) log P(y|x)$$
\( H(Y|X) = H(X,Y) – H(X)\) 
\(        =  \)
####相对熵  
####互信息  

## 概率分布的基本概念
#### 概率质量函数与概率密度函数 
概率质量函数和概率密度函数都可以理解为概率函数, 只是一个用于表达离散型取值对应概率的相关函数, 一个是用于表达连续性取值概率分布的相关函数.  

#### PDF：概率密度函数（probability density function）
 在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。

#### PMF : 概率质量函数（probability mass function)
 在概率论中，概率质量函数是离散随机变量在各特定取值上的概率。

#### CDF : 累积分布函数 (cumulative distribution function)
 概率密度函数的积分，能完整描述一个实随机变量X的概率分布。

#### 期望:
数学期望是随机变量的重要特征之一,随机变量X的数学期望记为E(X),E(X)是X的算术平均的近似值,数学期望表示了X的平均值大小。
- 当X为离散型随机变量时,并且其概率分布函数为\(P(X = x_k) = P_k\), 其中k = 1,2,...,n;则数学期望为:
  $$ E(x) = \sum_{k=1}^n x_k p_k $$
- 当X为连续型随机变量时,设其概率密度为f(x),则数学期望为
  $$E(x)= \int_{-\infty}^{+\infty}xf(x)dx$$
#### 方差:
数学期望给出了随机变量的平均大小,现实生活中我们还经常关心随机变量的取值在均值周围的散布程度,而方差就是这样的一个数字特征
设X是随机变量,并且\(E{[X-E(x)^2]}\)存在,则称它为X的方差,记为D(X)  

- 当X为离散型时: 
$$D(x) = \sum_k[x_k - E(X)]^2*P_k$$  
- 当X为连续型时: 
$$D(x) = \int_{-\infty}^{+\infty}[x_k - E(X)]^2*f(x)dx$$
#### 标准差:
- 方差的算术平方根\(\sigma(X) = \sqrt{D(x)} \)为X的标准差

- 另外,\(D(x) = E{[(X-E(x))^2]}\) 经过化解可得
  $$D(X) = E(X^2) – [E(X)]^2$$ 我们一般计算的时候常用这个式子。

#### 协方差:
用于表示两个离散数据各自偏离均值的程度, 如果A偏离均值的大小变大,B同时偏离军体的大小也变大的话,则是正相关,否则为负相关.


#### 相关系数:

#### 常见的概率分布:
各自的函数表示方法, 信息熵的推导与计算, 期望与方差的计算.  
##### 离散型分布
###### 伯努利分布:
伯努利试验是只有两种可能结果的单次随机试验，即对于一个随机变量X而言：
$$P_r[X = 1] = p$$
$$P_r[X = 0] = 1-p$$
概率质量函数:
$$f(x) = p^x(1-p)^{1-x} = \begin{cases}
   p &\text{if } x = 1 \\
   1 = p &\text{if } x = 0 \\
   0 &\text{otherwise}
\end{cases} $$

**伯努利分布的期望:**  \( p\)
**伯努利分布的方差:**  \( p * (1-p)\)

**推导过程如下:**
由于这里x只有两个取值所以该分布的数学**期望**:
$$E(X) = \sum_{i=0}^1x_if(x_i) = 1*p + 0 *(1-p) = p$$

方差则可以由方差公式来计算: \(D(x) = E(x^2) - (E(x))^2\)
该分布显然，\(x^2 = x\), 因此可以得到 \( E(x^2) = E(x) = p\)，
所以**方差**: \(D(x) = E(x^2) - (E(x))^2 = p - p^2 = p (1-p)\)

###### 二项分布:
二项分布(Binomial distribution)是n重伯努利试验成功次数的离散概率分布  
(1)做某件事情次数是固定的，次数用n表示，n次某件事相互独立
(2)每一次都有两个可能的结果（成功，失败）
(3)每一次成功的概率都相等，成功的概率用p表示
(4)想知道k次成功的概率



###### 多项式分布:
###### 离散型均匀分布:
###### 泊松分布

##### 连续型分布
###### 正态分布(高斯分布):
###### 指数分布:
###### 均匀分布:

##### 贝叶斯公式与自己的理解
##### 极大似然估计与自己的理解
